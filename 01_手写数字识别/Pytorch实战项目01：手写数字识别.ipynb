{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1+cu118'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST数据集手写数字识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.数据集介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST 包括了6万张28x28的训练样本，1万张测试样本，可以说它就是计算机视觉里面的Hello World。最关键的是使用cpu也可以跑，所以我们这里也会使用MNIST来进行实战。\n",
    "\n",
    "这里我们也自己从头搭建一个卷积神经网络，希望能够达到一个较高的准确率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.手写数字识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里先定义一些超参数，电脑性能不足的就减小批量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量大小\n",
    "batch_size = 512 \n",
    "# 总共训练批次\n",
    "epochs = 20 \n",
    "# 使用'cup'还是'gpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在torchvision这个包中就可以用于下载MNIST数据集，所以可以直接去使用。第一次下载需要一些时间下载，当然下载过了就不会再次下载了。\n",
    "\n",
    "这里我们直接使用DataLoader来对数据进行读取，后面会单独出一期关于DataLoader的知识。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 训练集与测试集下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 9912422/9912422 [02:45<00:00, 59854.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 28881/28881 [00:01<00:00, 17214.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 1648877/1648877 [00:26<00:00, 62048.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 4542/4542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data', train=True, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.自定义创建Model\n",
    "\n",
    "* 要先继承nn.Module且在其构造函数中需调用nn.Module的构造函数\n",
    "* 不用写反向传播函数，nn.Module能够利用autograd自动实现反向传播\n",
    "* Module中的可学习参数可以通过named_parameters()或者parameters()返回迭代器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们自定义一个网络，网络包含两个卷积层，分别是conv1和conv2，然后紧接着两个线性层作为输出，最后输出10个维度，这10个维度我们作为0-9的标识来确定识别出的是那个数字。\n",
    "\n",
    "每次会送入batch个样本，输入通道数1（黑白图像），图像分辨率是28x28，所以格式就是这样的 —— [batch,1,28,28]\n",
    "\n",
    "Conv2d: 第一个参数指输入通道数，第二个参数指输出通道数，第三个参数指卷积核的大小\n",
    "Linear：第一个参数指输入通道数，第二个参数指输出通道数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MnistNet,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5) \n",
    "        self.conv2 = nn.Conv2d(10, 20, 3) \n",
    "        self.fc1 = nn.Linear(20*10*10, 500) \n",
    "        self.fc2 = nn.Linear(500, 10) # 10分类\n",
    "        \n",
    "    def forward(self,x):\n",
    "        in_size = x.size(0) # BATCH_SIZE=512，输入的x可以看成是512*1*28*28的张量。\n",
    "        out = self.conv1(x) # batch*1*28*28 -> batch*10*24*24（28x28的图像经过一次核为5x5的卷积，输出变为24x24）\n",
    "        out = F.relu(out) # batch*10*24*24\n",
    "        out = F.max_pool2d(out, 2, 2) # batch*10*24*24 -> batch*10*12*12（2*2的池化层会减半）\n",
    "        out = self.conv2(out) # batch*10*12*12 -> batch*20*10*10（再卷积一次，核的大小是3）\n",
    "        out = F.relu(out) # batch*20*10*10\n",
    "        out = out.view(in_size, -1) # batch*20*10*10 -> batch*2000（out的第二维是-1，进行自动推算）\n",
    "        out = self.fc1(out) # batch*2000 -> batch*500\n",
    "        out = F.relu(out) # batch*500\n",
    "        out = self.fc2(out) # batch*500 -> batch*10\n",
    "        out = F.log_softmax(out, dim=1) # 计算log(softmax(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积的公式，最好记着：输出大小 = ((N - M + 2P) / S) + 1\n",
    "\n",
    "输入图像的大小是 N × N，卷积核的大小是 M × M，步幅（stride）为 S，填充（padding）为 P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印查看模型长什么样，是否正确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MnistNet(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=2000, out_features=500, bias=True)\n",
      "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = MnistNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以打印定义好名字里的权重和偏置项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight Parameter containing:\n",
      "tensor([[[[ 0.0443,  0.0222,  0.0170,  0.1916,  0.1348],\n",
      "          [-0.1929,  0.1519, -0.1807, -0.1214, -0.1014],\n",
      "          [ 0.1826,  0.0255, -0.0960,  0.0436,  0.1469],\n",
      "          [-0.1778,  0.0831, -0.0293, -0.0494, -0.1437],\n",
      "          [ 0.0892,  0.1882, -0.0203, -0.1730,  0.0614]]],\n",
      "\n",
      "\n",
      "        [[[-0.0362,  0.0380,  0.0790, -0.1535, -0.0465],\n",
      "          [ 0.0450, -0.1768, -0.0534,  0.0363,  0.0672],\n",
      "          [-0.0019, -0.0868,  0.1701,  0.1655,  0.0056],\n",
      "          [ 0.0413, -0.0175,  0.1054, -0.0454,  0.1005],\n",
      "          [ 0.1700,  0.1073,  0.0084,  0.1025, -0.0304]]],\n",
      "\n",
      "\n",
      "        [[[-0.0446,  0.0768,  0.1197,  0.0150, -0.0052],\n",
      "          [ 0.1346, -0.1450, -0.1986, -0.0118, -0.1841],\n",
      "          [-0.1355, -0.1696,  0.0133,  0.1899,  0.0200],\n",
      "          [-0.1700, -0.1997,  0.1026,  0.1700, -0.0515],\n",
      "          [-0.0114, -0.0995,  0.0858,  0.0654,  0.1008]]],\n",
      "\n",
      "\n",
      "        [[[-0.0822,  0.1187, -0.0238,  0.1033,  0.0478],\n",
      "          [-0.0369, -0.1912,  0.0380, -0.1130,  0.1713],\n",
      "          [-0.0300,  0.1939,  0.0009,  0.1141, -0.0816],\n",
      "          [ 0.1525, -0.0360,  0.1515,  0.0863, -0.0994],\n",
      "          [ 0.1618, -0.0060,  0.1751, -0.0247, -0.0783]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0505,  0.1875,  0.1181, -0.1299,  0.1599],\n",
      "          [-0.0084, -0.0079, -0.1969,  0.1417,  0.0999],\n",
      "          [-0.0346, -0.0092, -0.1396, -0.0684,  0.0573],\n",
      "          [-0.1564,  0.0292, -0.0391,  0.1768, -0.0390],\n",
      "          [ 0.1761, -0.1485,  0.1594,  0.1773,  0.1374]]],\n",
      "\n",
      "\n",
      "        [[[-0.0241, -0.1426, -0.1109,  0.1566,  0.1189],\n",
      "          [-0.0129, -0.1286,  0.1520, -0.0885,  0.0453],\n",
      "          [-0.1050,  0.1054,  0.0385, -0.1072,  0.1025],\n",
      "          [ 0.1614, -0.0331, -0.0514,  0.0649,  0.0963],\n",
      "          [ 0.1596, -0.1340, -0.1743,  0.0899,  0.1223]]],\n",
      "\n",
      "\n",
      "        [[[-0.0593,  0.0952, -0.0232, -0.1246,  0.1193],\n",
      "          [ 0.0314,  0.1985, -0.0120, -0.0479,  0.1865],\n",
      "          [ 0.1021,  0.0653, -0.1933, -0.1488, -0.1047],\n",
      "          [ 0.0983,  0.1603,  0.1107, -0.1748,  0.0833],\n",
      "          [ 0.1273, -0.0864,  0.1040, -0.0290,  0.0958]]],\n",
      "\n",
      "\n",
      "        [[[-0.0466,  0.0683, -0.1898,  0.1683, -0.1644],\n",
      "          [-0.1142, -0.1120,  0.0200, -0.1128, -0.1033],\n",
      "          [ 0.0900, -0.0904,  0.0537,  0.0264,  0.1292],\n",
      "          [ 0.1776, -0.0605, -0.0913, -0.0771,  0.1512],\n",
      "          [ 0.1041, -0.1663,  0.0328, -0.0885, -0.0502]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1843, -0.0363, -0.0956,  0.1436, -0.0795],\n",
      "          [-0.0820,  0.1637,  0.0072,  0.0066, -0.1435],\n",
      "          [ 0.0076, -0.0826,  0.1976,  0.0442, -0.0163],\n",
      "          [-0.0898, -0.0193, -0.0327,  0.0889,  0.1328],\n",
      "          [ 0.0981, -0.1522, -0.1051,  0.0728,  0.0700]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1217, -0.0402,  0.0625,  0.1286, -0.1982],\n",
      "          [ 0.1926,  0.1575,  0.0844, -0.1963,  0.0007],\n",
      "          [ 0.1566, -0.1680,  0.1818,  0.0528, -0.1031],\n",
      "          [ 0.1878,  0.0848,  0.1424, -0.0995,  0.0367],\n",
      "          [-0.0373, -0.0607, -0.1937,  0.0873,  0.1335]]]], requires_grad=True) torch.Size([10, 1, 5, 5])\n",
      "conv1.bias Parameter containing:\n",
      "tensor([ 0.1752, -0.1042,  0.1117,  0.0509, -0.0884, -0.1513,  0.0295, -0.1971,\n",
      "         0.0908,  0.1778], requires_grad=True) torch.Size([10])\n",
      "conv2.weight Parameter containing:\n",
      "tensor([[[[-9.7565e-02,  1.0228e-01, -4.4321e-02],\n",
      "          [ 3.4077e-02,  5.3934e-02,  7.4434e-02],\n",
      "          [ 2.7958e-02, -4.3590e-02,  9.2290e-04]],\n",
      "\n",
      "         [[-5.2184e-05,  9.6540e-02, -1.3208e-03],\n",
      "          [ 9.8144e-02,  7.2255e-02, -9.8857e-02],\n",
      "          [ 8.1350e-02, -3.0131e-02, -5.8163e-02]],\n",
      "\n",
      "         [[-8.8905e-02, -3.3999e-02,  9.7346e-02],\n",
      "          [-3.0151e-02, -8.8218e-02, -1.5696e-03],\n",
      "          [ 3.3428e-02, -5.1431e-02,  7.4763e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0201e-01, -9.3491e-02, -6.7286e-02],\n",
      "          [-7.9850e-02,  2.2420e-02,  5.5033e-02],\n",
      "          [-1.0159e-01,  2.3854e-02,  7.2290e-02]],\n",
      "\n",
      "         [[ 7.0765e-02,  8.8985e-02, -6.3773e-02],\n",
      "          [ 5.2945e-02, -4.6030e-02, -2.5221e-02],\n",
      "          [ 7.4912e-02, -4.3170e-02,  5.6691e-03]],\n",
      "\n",
      "         [[-3.3669e-03, -3.9549e-02, -6.4164e-02],\n",
      "          [ 4.5472e-04, -2.0750e-02, -3.5657e-02],\n",
      "          [ 1.0114e-02, -2.6885e-02, -2.7555e-02]]],\n",
      "\n",
      "\n",
      "        [[[-8.7112e-02, -1.6674e-02,  6.3425e-02],\n",
      "          [ 1.5121e-02,  3.0554e-02,  4.0396e-02],\n",
      "          [ 6.8577e-02,  1.6092e-02, -6.1823e-02]],\n",
      "\n",
      "         [[ 2.1463e-02,  9.4209e-03, -2.9389e-02],\n",
      "          [ 8.8422e-02, -2.8960e-02,  9.9515e-02],\n",
      "          [-4.3207e-02, -5.9738e-02,  4.2669e-02]],\n",
      "\n",
      "         [[-9.6201e-02, -1.0426e-01,  1.0092e-01],\n",
      "          [-2.7117e-02, -1.0301e-01,  5.5608e-02],\n",
      "          [-1.2426e-02, -3.1952e-02, -2.9761e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-9.5972e-02, -3.6447e-02, -8.4954e-03],\n",
      "          [ 5.2017e-02, -4.8520e-02,  8.9320e-02],\n",
      "          [-4.2867e-02,  4.4756e-03,  4.4215e-02]],\n",
      "\n",
      "         [[-5.1853e-02, -6.6057e-02, -3.0006e-02],\n",
      "          [-8.0631e-02,  4.2416e-03, -6.2055e-02],\n",
      "          [ 9.1491e-02,  4.5219e-02,  7.0046e-02]],\n",
      "\n",
      "         [[ 6.8025e-02, -9.8836e-03,  2.5672e-02],\n",
      "          [ 2.3566e-02, -9.2200e-02, -7.4510e-02],\n",
      "          [ 8.3977e-02,  9.6978e-02, -2.9676e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 8.5903e-02,  5.3971e-02, -4.2758e-02],\n",
      "          [ 6.9069e-02,  4.8408e-02,  8.6292e-02],\n",
      "          [ 5.7118e-03,  2.5955e-02, -6.7496e-02]],\n",
      "\n",
      "         [[-6.0943e-03, -3.4727e-02,  2.0126e-02],\n",
      "          [ 3.5874e-02,  2.4404e-04, -7.0262e-02],\n",
      "          [-7.6813e-02, -9.8923e-02,  6.9196e-02]],\n",
      "\n",
      "         [[-1.7340e-02, -5.4216e-02,  1.7375e-02],\n",
      "          [-9.9001e-02, -2.2971e-02,  6.3050e-02],\n",
      "          [-2.7538e-02,  5.7793e-02, -7.1355e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.7575e-02, -6.1516e-02, -7.3135e-02],\n",
      "          [ 2.0200e-02, -8.2526e-02,  3.1972e-02],\n",
      "          [ 2.5649e-02,  5.6416e-02,  9.7037e-02]],\n",
      "\n",
      "         [[ 9.8227e-02, -4.2288e-02, -3.0846e-02],\n",
      "          [ 4.8474e-02,  6.9333e-03,  6.9951e-02],\n",
      "          [-1.2543e-02,  2.6956e-02, -8.1589e-02]],\n",
      "\n",
      "         [[ 6.4638e-02,  5.3575e-02,  5.2282e-02],\n",
      "          [ 3.4375e-02, -7.6570e-02, -2.6498e-02],\n",
      "          [ 2.2409e-02, -1.2384e-02, -7.9725e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 5.7754e-03, -3.4548e-02, -6.7188e-02],\n",
      "          [ 7.9767e-02,  3.1931e-02, -7.1230e-02],\n",
      "          [-1.0109e-01,  4.1774e-02, -8.2352e-02]],\n",
      "\n",
      "         [[ 9.3704e-02, -9.9436e-02,  4.0050e-02],\n",
      "          [-9.8673e-02, -1.6193e-02, -8.1885e-02],\n",
      "          [-8.4592e-02, -8.5385e-02,  1.1649e-02]],\n",
      "\n",
      "         [[ 7.1037e-02,  7.6269e-02, -7.2580e-02],\n",
      "          [ 3.8237e-02,  4.0482e-02,  1.0058e-01],\n",
      "          [ 6.7113e-02,  3.0037e-02,  1.6736e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.4570e-02, -3.0733e-02,  7.0589e-02],\n",
      "          [-5.5846e-02,  1.2214e-02, -3.5807e-02],\n",
      "          [-2.5065e-02,  4.5393e-02,  2.8970e-02]],\n",
      "\n",
      "         [[-6.1201e-02,  6.3830e-02,  3.5669e-02],\n",
      "          [-1.1940e-02, -9.8710e-02,  8.1945e-02],\n",
      "          [ 1.0326e-01,  1.4574e-03,  3.5086e-02]],\n",
      "\n",
      "         [[ 6.7410e-02,  2.3229e-02, -4.1573e-02],\n",
      "          [-2.5247e-02, -8.9671e-02, -4.8835e-02],\n",
      "          [ 9.3421e-02,  3.3186e-02, -5.3188e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0162e-01,  3.4998e-02,  6.9939e-03],\n",
      "          [-4.1074e-02,  8.0873e-02,  7.9662e-02],\n",
      "          [ 2.0212e-02,  1.0280e-01, -7.5905e-02]],\n",
      "\n",
      "         [[-9.6704e-02,  8.4412e-02, -8.0823e-02],\n",
      "          [-1.0163e-01, -6.9083e-02,  4.3923e-02],\n",
      "          [-8.7857e-02, -4.6207e-03, -8.5496e-02]],\n",
      "\n",
      "         [[-3.4807e-02, -7.6476e-02, -5.0487e-02],\n",
      "          [-7.1576e-02, -1.0497e-01, -1.4738e-02],\n",
      "          [ 4.7236e-02,  6.9863e-02, -9.8439e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.5771e-02,  1.2501e-02,  7.6897e-02],\n",
      "          [-9.1819e-02, -8.1179e-02,  9.8897e-02],\n",
      "          [ 5.3639e-02, -9.8616e-02,  8.0505e-02]],\n",
      "\n",
      "         [[ 5.2125e-02, -3.4572e-02, -2.1884e-02],\n",
      "          [-1.0227e-01, -4.6363e-02, -1.0045e-01],\n",
      "          [ 9.4715e-03,  2.1045e-02, -2.9393e-02]],\n",
      "\n",
      "         [[ 3.8278e-02, -8.4717e-02,  2.8332e-02],\n",
      "          [ 9.2839e-02,  4.4105e-02,  8.0378e-02],\n",
      "          [ 7.9805e-02, -5.5224e-02, -6.2252e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.3760e-03,  1.1965e-02, -9.2006e-02],\n",
      "          [ 7.3806e-02,  6.6810e-02,  4.3511e-02],\n",
      "          [ 7.3821e-03,  9.1146e-02, -1.0264e-01]],\n",
      "\n",
      "         [[ 7.7483e-02,  3.3279e-02, -3.4129e-02],\n",
      "          [ 7.7628e-02, -5.2037e-02,  2.8360e-02],\n",
      "          [ 7.2998e-02,  1.0760e-02, -1.0025e-01]],\n",
      "\n",
      "         [[-9.5571e-02, -7.8796e-02,  9.9954e-02],\n",
      "          [-1.0460e-01,  3.7997e-02,  1.2790e-02],\n",
      "          [-8.3676e-02, -7.8648e-03, -3.8480e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.9365e-02, -4.7356e-02, -6.7219e-02],\n",
      "          [ 9.3707e-02, -1.4743e-02,  1.0312e-01],\n",
      "          [ 9.4649e-02,  9.4947e-02,  8.6157e-02]],\n",
      "\n",
      "         [[-4.6334e-02, -2.3893e-02,  2.5614e-02],\n",
      "          [ 4.8732e-02, -9.9745e-02, -6.4109e-02],\n",
      "          [ 7.0415e-02, -9.2886e-02, -1.0181e-01]],\n",
      "\n",
      "         [[ 4.9732e-02,  6.3350e-02, -9.4104e-02],\n",
      "          [ 6.1855e-02,  1.0171e-01, -3.8809e-02],\n",
      "          [-6.0092e-03,  6.5267e-02,  9.6853e-02]]]], requires_grad=True) torch.Size([20, 10, 3, 3])\n",
      "conv2.bias Parameter containing:\n",
      "tensor([-0.0715,  0.0319, -0.1005,  0.0436,  0.0339,  0.0500, -0.0646,  0.0359,\n",
      "         0.0444,  0.0510, -0.0465,  0.0615, -0.0774,  0.0611,  0.0754,  0.0163,\n",
      "         0.0881,  0.0313,  0.0868, -0.0594], requires_grad=True) torch.Size([20])\n",
      "fc1.weight Parameter containing:\n",
      "tensor([[-0.0116,  0.0124,  0.0169,  ...,  0.0003, -0.0201, -0.0165],\n",
      "        [ 0.0077, -0.0125, -0.0099,  ..., -0.0158, -0.0127, -0.0096],\n",
      "        [ 0.0041, -0.0196, -0.0063,  ..., -0.0003,  0.0033,  0.0094],\n",
      "        ...,\n",
      "        [ 0.0142,  0.0219, -0.0017,  ...,  0.0163, -0.0114,  0.0076],\n",
      "        [ 0.0023, -0.0020,  0.0141,  ..., -0.0054, -0.0106, -0.0105],\n",
      "        [-0.0174, -0.0127, -0.0197,  ...,  0.0148,  0.0121,  0.0074]],\n",
      "       requires_grad=True) torch.Size([500, 2000])\n",
      "fc1.bias Parameter containing:\n",
      "tensor([ 1.1814e-02, -1.6771e-03,  2.1101e-03,  9.6948e-03,  1.7226e-02,\n",
      "        -1.2045e-02, -2.0513e-02, -9.1491e-03,  2.2126e-03, -1.0390e-02,\n",
      "         4.4678e-03,  8.4362e-03, -1.2884e-02,  8.4916e-03, -3.3958e-03,\n",
      "        -6.4173e-03, -1.8111e-02,  6.1153e-03, -7.4587e-03, -2.0615e-02,\n",
      "        -7.4118e-03,  1.8979e-02,  2.1461e-02,  4.1989e-03, -1.6408e-02,\n",
      "         9.9086e-03, -2.0928e-03,  2.8618e-04,  2.0250e-02, -1.6703e-02,\n",
      "         1.1565e-02, -8.5218e-04,  7.2011e-03,  1.2563e-02,  8.2903e-03,\n",
      "         6.3422e-03,  2.2045e-03,  2.2231e-03,  1.5181e-03,  1.4145e-02,\n",
      "         3.7165e-03, -1.9958e-02,  8.5192e-03, -3.0162e-03,  1.9277e-02,\n",
      "         1.7421e-02, -1.5178e-02, -6.1261e-04,  1.4321e-02,  6.8273e-03,\n",
      "         1.5363e-02,  7.9114e-03,  1.4581e-02,  5.3897e-03, -7.1626e-03,\n",
      "        -1.5037e-02, -2.7498e-04, -7.3431e-03,  2.0801e-02, -8.3342e-03,\n",
      "         3.5417e-03,  2.0378e-02, -2.0539e-02,  1.1295e-02, -2.4268e-03,\n",
      "        -6.4490e-03,  5.6585e-03, -1.4271e-02,  1.1938e-02, -1.8095e-03,\n",
      "        -8.8883e-03,  9.8983e-03,  5.4163e-03,  2.0741e-03, -1.8297e-02,\n",
      "         1.0322e-02, -2.2295e-02, -1.7970e-02,  9.3126e-03,  9.5672e-03,\n",
      "        -1.2632e-02,  4.2298e-03, -5.0017e-03,  8.9431e-03, -1.4248e-02,\n",
      "        -1.5766e-03, -8.4223e-03,  5.9397e-03,  1.9594e-02,  1.4819e-02,\n",
      "        -9.8441e-04,  9.1645e-03,  2.0256e-02, -1.7676e-02, -4.9008e-03,\n",
      "        -1.0221e-02,  2.3381e-03, -1.4839e-02,  6.7174e-03, -1.3014e-02,\n",
      "         4.8500e-03, -1.0976e-02, -2.0650e-02,  1.5314e-02, -6.9252e-03,\n",
      "        -1.3141e-02, -4.1289e-03,  7.7842e-03, -5.9027e-03, -1.9138e-02,\n",
      "        -9.4413e-03, -1.5192e-02, -5.1605e-03,  6.8118e-03, -1.4953e-02,\n",
      "         8.2998e-03,  4.9802e-03,  3.0995e-03,  1.5362e-02,  1.4514e-02,\n",
      "         5.3591e-03,  2.0138e-02, -1.9085e-02, -1.0859e-02,  1.7706e-02,\n",
      "         4.9398e-03, -3.6840e-03, -2.1028e-02,  7.6714e-03,  1.4648e-02,\n",
      "         1.8515e-02, -5.9203e-04,  1.0238e-02,  6.3687e-03,  1.4232e-03,\n",
      "        -9.4241e-03,  3.8742e-03,  1.7632e-02,  7.4014e-03,  8.3825e-03,\n",
      "        -1.6563e-02,  2.0105e-02, -9.9780e-03,  1.2996e-02, -7.3660e-03,\n",
      "        -1.1071e-03, -8.3544e-03,  1.5158e-02,  5.9724e-03, -2.1292e-02,\n",
      "         9.8802e-04, -1.1312e-03, -1.1165e-02,  6.5652e-03,  1.9249e-02,\n",
      "        -2.0935e-02,  5.2462e-03,  6.2922e-03,  1.0647e-02, -2.2228e-02,\n",
      "         1.4227e-02, -1.9315e-02, -1.5023e-02, -1.2056e-02,  1.3803e-02,\n",
      "         1.0126e-02,  5.9040e-03, -1.9763e-02,  6.3776e-03, -1.8534e-02,\n",
      "         2.0465e-02,  1.5199e-02,  1.3134e-02,  7.3268e-03,  2.7047e-03,\n",
      "        -2.7862e-03,  1.9517e-02,  2.1747e-02,  1.3902e-02,  2.3024e-03,\n",
      "        -4.5136e-03,  5.1216e-03, -1.3743e-03,  1.9167e-02, -1.4332e-02,\n",
      "        -2.9871e-03, -7.8635e-04, -7.7190e-03,  1.7011e-02, -3.5094e-03,\n",
      "         4.1833e-03, -4.6987e-03, -2.0249e-02, -2.0845e-03,  8.9963e-03,\n",
      "        -1.1430e-02, -8.7643e-03, -1.8929e-02,  1.4961e-02, -1.3139e-02,\n",
      "        -5.1486e-03, -2.7385e-03, -1.7644e-02,  9.1447e-03, -1.2105e-02,\n",
      "         1.5283e-02,  5.1094e-03,  2.1798e-02,  2.0272e-02, -1.6897e-02,\n",
      "         5.5553e-03, -1.6724e-02, -6.1501e-03, -2.7861e-03,  1.9717e-03,\n",
      "        -6.5322e-03,  1.5918e-02,  5.8063e-03,  1.8093e-02,  2.1664e-03,\n",
      "        -7.6379e-03, -6.6367e-04,  2.1567e-02, -1.2184e-02,  1.2563e-02,\n",
      "         7.6139e-03, -2.0395e-02,  6.1880e-03,  8.8431e-03,  5.4762e-03,\n",
      "        -1.7751e-02, -6.2191e-04, -2.0415e-02,  1.5340e-02, -8.5249e-03,\n",
      "        -1.0657e-02, -9.8653e-03, -1.3627e-02, -6.5487e-03,  1.8470e-02,\n",
      "        -1.8884e-02, -1.0824e-02, -1.5160e-02, -1.7194e-02,  1.3602e-02,\n",
      "         1.6291e-02, -5.5255e-03,  1.0931e-02, -6.8425e-03, -3.4190e-03,\n",
      "        -2.1038e-02,  2.1108e-02,  5.3500e-04, -3.4241e-03, -7.2396e-03,\n",
      "        -1.2723e-02, -1.7720e-02,  4.8944e-03, -7.9335e-03, -1.6123e-02,\n",
      "        -1.2078e-02, -1.7589e-02,  1.9038e-02,  1.8470e-02, -1.9868e-02,\n",
      "         1.5948e-02, -2.0168e-02,  1.8340e-02,  8.4450e-03, -1.0015e-02,\n",
      "        -1.7583e-02,  2.0427e-02, -8.8557e-03,  1.7738e-02, -4.8091e-03,\n",
      "        -1.8795e-02, -5.0906e-03,  1.9979e-02, -1.3303e-02, -2.0444e-02,\n",
      "        -3.5886e-04,  8.4965e-03,  1.2734e-02,  1.3498e-03, -9.2107e-03,\n",
      "        -4.3053e-04,  1.4523e-02,  1.8585e-02, -1.7346e-02, -3.1440e-03,\n",
      "         1.4178e-03,  1.1475e-03, -1.4026e-02,  1.1126e-02,  1.6792e-02,\n",
      "        -5.1213e-03,  1.4072e-03, -1.6078e-03,  8.3967e-03,  2.0521e-02,\n",
      "         9.9865e-03, -1.2337e-02,  1.0719e-02,  2.1236e-02,  1.2729e-02,\n",
      "         2.0076e-02,  1.0267e-02,  5.4903e-03, -1.5205e-02,  1.7411e-02,\n",
      "         5.9940e-03,  2.0559e-03,  1.4909e-02, -2.0308e-02,  2.0415e-02,\n",
      "        -1.5290e-02,  1.9880e-02, -5.0942e-03, -1.6238e-02,  7.2952e-03,\n",
      "         2.1346e-02,  1.4477e-02, -1.8419e-02,  7.4365e-04, -1.9495e-02,\n",
      "         1.4435e-02, -1.4115e-02, -1.6679e-02,  6.5318e-03, -8.9328e-03,\n",
      "         9.4940e-03, -4.1929e-03,  6.7499e-03, -8.9012e-03, -9.3473e-04,\n",
      "        -1.3354e-02, -1.7860e-02,  6.4301e-03,  2.1658e-02, -9.8624e-03,\n",
      "         1.6062e-02, -1.7208e-02,  8.3486e-03,  2.7841e-03, -1.7775e-02,\n",
      "        -1.6367e-02, -2.0529e-02, -1.7388e-02, -5.9876e-03,  1.6625e-02,\n",
      "         2.0615e-02,  6.2614e-03,  1.2019e-02, -1.0906e-02,  1.7473e-03,\n",
      "         1.7076e-02,  3.5075e-03, -5.5412e-03,  1.4744e-02, -2.0055e-02,\n",
      "         9.4724e-03, -3.4793e-03, -2.1200e-02,  1.7800e-02, -1.2328e-02,\n",
      "        -2.1990e-02,  7.0355e-03,  1.6729e-02, -1.5433e-02, -1.3588e-02,\n",
      "         1.1029e-02,  2.0773e-02,  5.7976e-03, -1.0509e-02, -1.9400e-02,\n",
      "         1.1894e-02,  1.4093e-02,  2.0147e-02,  2.0531e-02,  1.7073e-03,\n",
      "         2.2285e-02, -2.2840e-03, -1.9098e-02,  3.1823e-03, -4.1603e-04,\n",
      "        -2.4487e-03,  1.5377e-02,  3.0021e-03, -1.4577e-03, -1.9447e-02,\n",
      "         1.9043e-02, -2.2267e-02,  5.5009e-03, -1.4339e-02, -1.1006e-02,\n",
      "        -1.3256e-02,  1.2749e-02,  1.6668e-02,  2.1293e-02, -1.4322e-02,\n",
      "         1.0641e-02, -2.1878e-02, -3.2839e-03, -4.9063e-03, -1.4414e-02,\n",
      "         2.8988e-03,  1.2857e-02,  1.9857e-02,  4.7753e-03, -1.0515e-02,\n",
      "        -5.2021e-03, -2.8323e-03,  6.3585e-03, -1.0106e-02, -3.4937e-03,\n",
      "        -3.2858e-03, -1.9019e-02,  8.8154e-03,  6.6318e-03, -6.8633e-03,\n",
      "        -1.5859e-02,  8.8496e-04, -2.1037e-02,  1.6306e-02,  1.7384e-02,\n",
      "        -3.4340e-03, -9.3598e-05, -7.2094e-03, -1.2365e-02,  8.8103e-03,\n",
      "         1.3325e-02, -1.2027e-02,  4.5005e-03, -1.1133e-02, -9.6860e-03,\n",
      "        -9.8984e-03, -1.6481e-03,  2.1668e-02,  2.2215e-02,  8.2657e-03,\n",
      "         5.6227e-03,  1.0087e-02,  1.7845e-02, -2.7138e-04, -9.0111e-03,\n",
      "        -1.2311e-02, -1.7146e-02, -1.6740e-02,  1.6541e-02,  3.3453e-03,\n",
      "         7.3669e-03,  1.0827e-03,  1.6699e-02, -2.0942e-02,  6.0250e-03,\n",
      "         1.4901e-02,  1.6700e-02, -4.9177e-03, -2.1396e-02,  1.1147e-03,\n",
      "         9.4555e-03,  2.1825e-02, -2.0215e-02, -1.3347e-03,  9.6783e-03,\n",
      "         2.1994e-02, -2.7084e-03,  1.9332e-02, -4.5831e-03, -1.5141e-02,\n",
      "         6.3420e-03,  2.0064e-02,  1.8968e-02,  7.6618e-04,  7.3900e-03,\n",
      "        -1.3942e-02,  5.7106e-03,  6.0609e-03,  1.4602e-02,  3.3886e-03,\n",
      "        -8.7787e-03, -1.3090e-02, -1.1001e-03, -6.6471e-03,  1.0278e-02,\n",
      "         2.0645e-02,  1.7360e-02, -6.4365e-03, -1.2139e-02,  4.4165e-03,\n",
      "        -4.3566e-03,  9.1444e-03, -7.0901e-03, -1.7722e-02,  1.0914e-02,\n",
      "        -9.7844e-03, -5.1254e-03,  8.7077e-03, -1.7289e-02, -1.1394e-02],\n",
      "       requires_grad=True) torch.Size([500])\n",
      "fc2.weight Parameter containing:\n",
      "tensor([[-0.0266,  0.0353,  0.0303,  ...,  0.0418,  0.0411, -0.0163],\n",
      "        [-0.0436,  0.0265,  0.0406,  ..., -0.0258,  0.0408, -0.0045],\n",
      "        [-0.0084, -0.0206, -0.0211,  ..., -0.0011, -0.0332,  0.0287],\n",
      "        ...,\n",
      "        [-0.0136,  0.0261,  0.0359,  ...,  0.0400,  0.0030, -0.0139],\n",
      "        [-0.0198,  0.0073,  0.0064,  ..., -0.0243,  0.0280, -0.0355],\n",
      "        [-0.0315,  0.0429, -0.0099,  ...,  0.0057, -0.0235,  0.0291]],\n",
      "       requires_grad=True) torch.Size([10, 500])\n",
      "fc2.bias Parameter containing:\n",
      "tensor([ 0.0239,  0.0060,  0.0440,  0.0090, -0.0119,  0.0287, -0.0056,  0.0057,\n",
      "         0.0104, -0.0322], requires_grad=True) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, parameter in net.named_parameters():\n",
    "    print(name, parameter,parameter.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例化后使用.to方法将网络移动到GPU\n",
    "\n",
    "优化器就选择最有效的Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(device)\n",
    "optimizer = optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后需要定义一个train函数，这里面封装着我的一些训练的操作，损失函数这里用的是负对数似然损失函数，你可以自己替换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if(batch_idx+1)%30 == 0: \n",
    "            print('当前Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试的操作几乎相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # 将一批的损失相加\n",
    "            pred = output.max(1, keepdim=True)[1] # 找到概率最大的下标\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    acc = correct / len(test_loader.dataset) * 100. \n",
    "    print('\\n验证集: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, \n",
    "        len(test_loader.dataset), acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来训练也是一步到位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Train Epoch: 1 [14848/60000 (25%)]\tLoss: 0.317121\n",
      "当前Train Epoch: 1 [30208/60000 (50%)]\tLoss: 0.194249\n",
      "当前Train Epoch: 1 [45568/60000 (75%)]\tLoss: 0.175830\n",
      "\n",
      "验证集: Average loss: 0.1077, Accuracy: 9689/10000 (97%)\n",
      "\n",
      "当前Train Epoch: 2 [14848/60000 (25%)]\tLoss: 0.119607\n",
      "当前Train Epoch: 2 [30208/60000 (50%)]\tLoss: 0.080488\n",
      "当前Train Epoch: 2 [45568/60000 (75%)]\tLoss: 0.075732\n",
      "\n",
      "验证集: Average loss: 0.0659, Accuracy: 9791/10000 (98%)\n",
      "\n",
      "当前Train Epoch: 3 [14848/60000 (25%)]\tLoss: 0.053178\n",
      "当前Train Epoch: 3 [30208/60000 (50%)]\tLoss: 0.051680\n",
      "当前Train Epoch: 3 [45568/60000 (75%)]\tLoss: 0.033485\n",
      "\n",
      "验证集: Average loss: 0.0484, Accuracy: 9836/10000 (98%)\n",
      "\n",
      "当前Train Epoch: 4 [14848/60000 (25%)]\tLoss: 0.049852\n",
      "当前Train Epoch: 4 [30208/60000 (50%)]\tLoss: 0.034732\n",
      "当前Train Epoch: 4 [45568/60000 (75%)]\tLoss: 0.055895\n",
      "\n",
      "验证集: Average loss: 0.0477, Accuracy: 9837/10000 (98%)\n",
      "\n",
      "当前Train Epoch: 5 [14848/60000 (25%)]\tLoss: 0.029753\n",
      "当前Train Epoch: 5 [30208/60000 (50%)]\tLoss: 0.047640\n",
      "当前Train Epoch: 5 [45568/60000 (75%)]\tLoss: 0.016334\n",
      "\n",
      "验证集: Average loss: 0.0410, Accuracy: 9866/10000 (99%)\n",
      "\n",
      "当前Train Epoch: 6 [14848/60000 (25%)]\tLoss: 0.027817\n",
      "当前Train Epoch: 6 [30208/60000 (50%)]\tLoss: 0.040083\n",
      "当前Train Epoch: 6 [45568/60000 (75%)]\tLoss: 0.034269\n",
      "\n",
      "验证集: Average loss: 0.0403, Accuracy: 9866/10000 (99%)\n",
      "\n",
      "当前Train Epoch: 7 [14848/60000 (25%)]\tLoss: 0.012236\n",
      "当前Train Epoch: 7 [30208/60000 (50%)]\tLoss: 0.021643\n",
      "当前Train Epoch: 7 [45568/60000 (75%)]\tLoss: 0.016333\n",
      "\n",
      "验证集: Average loss: 0.0352, Accuracy: 9887/10000 (99%)\n",
      "\n",
      "当前Train Epoch: 8 [14848/60000 (25%)]\tLoss: 0.013635\n",
      "当前Train Epoch: 8 [30208/60000 (50%)]\tLoss: 0.017525\n",
      "当前Train Epoch: 8 [45568/60000 (75%)]\tLoss: 0.011934\n",
      "\n",
      "验证集: Average loss: 0.0310, Accuracy: 9903/10000 (99%)\n",
      "\n",
      "当前Train Epoch: 9 [14848/60000 (25%)]\tLoss: 0.009698\n",
      "当前Train Epoch: 9 [30208/60000 (50%)]\tLoss: 0.012087\n",
      "当前Train Epoch: 9 [45568/60000 (75%)]\tLoss: 0.010395\n",
      "\n",
      "验证集: Average loss: 0.0396, Accuracy: 9877/10000 (99%)\n",
      "\n",
      "当前Train Epoch: 10 [14848/60000 (25%)]\tLoss: 0.011283\n",
      "当前Train Epoch: 10 [30208/60000 (50%)]\tLoss: 0.008606\n",
      "当前Train Epoch: 10 [45568/60000 (75%)]\tLoss: 0.016871\n",
      "\n",
      "验证集: Average loss: 0.0368, Accuracy: 9891/10000 (99%)\n",
      "\n",
      "当前Train Epoch: 11 [14848/60000 (25%)]\tLoss: 0.003133\n",
      "当前Train Epoch: 11 [30208/60000 (50%)]\tLoss: 0.025135\n",
      "当前Train Epoch: 11 [45568/60000 (75%)]\tLoss: 0.007189\n",
      "\n",
      "验证集: Average loss: 0.0336, Accuracy: 9891/10000 (99%)\n",
      "\n",
      "当前Train Epoch: 12 [14848/60000 (25%)]\tLoss: 0.014229\n",
      "当前Train Epoch: 12 [30208/60000 (50%)]\tLoss: 0.015453\n",
      "当前Train Epoch: 12 [45568/60000 (75%)]\tLoss: 0.010085\n",
      "\n",
      "验证集: Average loss: 0.0413, Accuracy: 9885/10000 (99%)\n",
      "\n",
      "当前Train Epoch: 13 [14848/60000 (25%)]\tLoss: 0.008614\n",
      "当前Train Epoch: 13 [30208/60000 (50%)]\tLoss: 0.009379\n",
      "当前Train Epoch: 13 [45568/60000 (75%)]\tLoss: 0.008568\n",
      "\n",
      "验证集: Average loss: 0.0367, Accuracy: 9895/10000 (99%)\n",
      "\n",
      "当前Train Epoch: 14 [14848/60000 (25%)]\tLoss: 0.001740\n",
      "当前Train Epoch: 14 [30208/60000 (50%)]\tLoss: 0.002038\n",
      "当前Train Epoch: 14 [45568/60000 (75%)]\tLoss: 0.002611\n",
      "\n",
      "验证集: Average loss: 0.0349, Accuracy: 9902/10000 (99%)\n",
      "\n",
      "当前Train Epoch: 15 [14848/60000 (25%)]\tLoss: 0.001619\n",
      "当前Train Epoch: 15 [30208/60000 (50%)]\tLoss: 0.001688\n",
      "当前Train Epoch: 15 [45568/60000 (75%)]\tLoss: 0.016366\n",
      "\n",
      "验证集: Average loss: 0.0307, Accuracy: 9921/10000 (99%)\n",
      "\n",
      "当前Train Epoch: 16 [14848/60000 (25%)]\tLoss: 0.003028\n",
      "当前Train Epoch: 16 [30208/60000 (50%)]\tLoss: 0.001793\n",
      "当前Train Epoch: 16 [45568/60000 (75%)]\tLoss: 0.002427\n",
      "\n",
      "验证集: Average loss: 0.0377, Accuracy: 9897/10000 (99%)\n",
      "\n",
      "当前Train Epoch: 17 [14848/60000 (25%)]\tLoss: 0.000749\n",
      "当前Train Epoch: 17 [30208/60000 (50%)]\tLoss: 0.000504\n",
      "当前Train Epoch: 17 [45568/60000 (75%)]\tLoss: 0.000985\n",
      "\n",
      "验证集: Average loss: 0.0378, Accuracy: 9909/10000 (99%)\n",
      "\n",
      "当前Train Epoch: 18 [14848/60000 (25%)]\tLoss: 0.001550\n",
      "当前Train Epoch: 18 [30208/60000 (50%)]\tLoss: 0.003889\n",
      "当前Train Epoch: 18 [45568/60000 (75%)]\tLoss: 0.000351\n",
      "\n",
      "验证集: Average loss: 0.0361, Accuracy: 9906/10000 (99%)\n",
      "\n",
      "当前Train Epoch: 19 [14848/60000 (25%)]\tLoss: 0.000943\n",
      "当前Train Epoch: 19 [30208/60000 (50%)]\tLoss: 0.000738\n",
      "当前Train Epoch: 19 [45568/60000 (75%)]\tLoss: 0.000190\n",
      "\n",
      "验证集: Average loss: 0.0412, Accuracy: 9899/10000 (99%)\n",
      "\n",
      "当前Train Epoch: 20 [14848/60000 (25%)]\tLoss: 0.000675\n",
      "当前Train Epoch: 20 [30208/60000 (50%)]\tLoss: 0.000763\n",
      "当前Train Epoch: 20 [45568/60000 (75%)]\tLoss: 0.000325\n",
      "\n",
      "验证集: Average loss: 0.0385, Accuracy: 9906/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(net, device, train_loader, optimizer, epoch)\n",
    "    test(net, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然跑的不多，只有区区20轮，不过准确率也还是相当的高的，最终准确率也是达到了99%\n",
    "\n",
    "深度学习中的手写数字识别是一个常见的入门项目，它可以帮助初学者熟悉神经网络的基本概念和训练过程。通过实践手写数字识别，可以学习到数据预理、模型构建、训练和评估等关键步骤。\n",
    "\n",
    "但并不是说的你的模型就特别好，如果你搭建的模型连Minist都搞不定，那你的模型一定不是很好。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch(2.0)",
   "language": "python",
   "name": "torch1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
